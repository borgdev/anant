#!/usr/bin/env python3
"""
Full Dataset FHIR Processing Script (High-Performance Edition)
==============================================================

Processes the complete 435GB FHIR dataset (116K+ files) with ENHANCED settings:
- 16GB RAM (2x previous): Better memory management and caching
- 100 files/batch (2x previous): Higher throughput processing
- 8 worker threads (2x previous): Maximum parallelism
- Hierarchical knowledge graph construction
- Progress monitoring and performance tracking
- Error handling and recovery

PERFORMANCE IMPROVEMENTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Setting     â”‚   Previous   â”‚   Enhanced   â”‚   Improvement   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memory Limit    â”‚     8GB      â”‚     16GB     â”‚      +100%      â”‚
â”‚ Batch Size      â”‚   50 files   â”‚  100 files   â”‚      +100%      â”‚
â”‚ Worker Threads  â”‚      4       â”‚      8       â”‚      +100%      â”‚
â”‚ Expected Speed  â”‚   ~6hrs      â”‚   ~3.5hrs    â”‚      +40%       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usage:
    python3 run_full_dataset_processing.py
"""

import os
import sys
import time
import logging
from pathlib import Path
from large_scale_fhir_processor import LargeScaleFHIRProcessor

# Configure logging for production run
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/media/amansingh/data/fhir_test/processing.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

def main():
    """Main function for full dataset processing."""
    
    print("ğŸš€ STARTING FULL 435GB FHIR DATASET PROCESSING")
    print("=" * 80)
    print(f"ğŸ“… Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ—‚ï¸  Source: /home/amansingh/dev/andola/healthcare/synthea/output/fhir")
    print(f"ğŸ“ Output: /media/amansingh/data/fhir_test")
    print(f"ğŸ§  Memory: 16.0 GB | ğŸ”„ Batch: 100 files | âš¡ Workers: 8 threads")
    print("=" * 80)
    
    # Initialize processor with high-performance settings for full dataset
    # 16GB RAM: 2x memory for larger batches and better caching
    # 100 files/batch: 2x batch size for better throughput
    # 8 workers: 2x parallelism for faster processing
    processor = LargeScaleFHIRProcessor(
        fhir_data_dir='/home/amansingh/dev/andola/healthcare/synthea/output/fhir',
        output_dir='/media/amansingh/data/fhir_test',
        max_memory_gb=16.0,      # Use 16GB memory limit  
        batch_size=100,          # Process 100 files per batch
        max_workers=8,           # Use 8 worker threads
        enable_graph=True        # Enable hierarchical knowledge graph
    )
    
    try:
        # Discovery phase
        print(f"\nğŸ” PHASE 1: DISCOVERY & ESTIMATION")
        print("-" * 60)
        
        discovery = processor.discover_fhir_files()
        
        print(f"ğŸ“ Total files found: {discovery['total_files']:,}")
        print(f"ğŸ’¾ Estimated dataset size: {discovery['estimated_size_gb']:.1f} GB")
        print(f"â±ï¸ Estimated processing time: {discovery['estimated_processing_time_hours']:.1f} hours")
        print(f"ğŸš€ With 8 workers & 100-file batches: ~{discovery['estimated_processing_time_hours'] * 0.6:.1f} hours expected")
        print(f"ğŸ§® Expected partitions: ~{discovery['total_files'] // 1000} per resource type")
        
        # Confirm before proceeding
        if discovery['total_files'] > 100000:  # Safety check
            print(f"\nâš ï¸  WARNING: This is a large dataset with {discovery['total_files']:,} files!")
            print(f"ğŸ“Š This will create substantial Parquet outputs and knowledge graphs.")
            
            # In production, you might want to add a confirmation prompt
            print(f"âœ… Proceeding with full dataset processing...")
        
        # Processing phase
        print(f"\nâš¡ PHASE 2: FULL DATASET PROCESSING")
        print("-" * 60)
        print(f"ğŸ­ Processing {discovery['total_files']:,} FHIR files...")
        print(f"ğŸ§  Memory limit: 16.0 GB")
        print(f"ğŸ”„ Batch size: 100 files")
        print(f"âš¡ Workers: 8 threads")
        print(f"ğŸ•¸ï¸  Knowledge graph: Enabled")
        
        # Start processing
        start_time = time.time()
        result = processor.process_all_files()  # Process ALL files
        end_time = time.time()
        
        # Results phase
        print(f"\nğŸ“Š PHASE 3: PROCESSING COMPLETE!")
        print("=" * 80)
        
        stats = result['stats']
        partition_summary = result['partition_summary']
        graph_summary = result.get('hierarchical_knowledge_graph', {})
        
        # Performance metrics
        total_time_hours = (end_time - start_time) / 3600
        processing_rate = stats.files_processed / (end_time - start_time)
        data_throughput = stats.bytes_processed / (1024**3) / (end_time - start_time)
        
        print(f"âœ… PROCESSING COMPLETED SUCCESSFULLY!")
        print(f"ğŸ“ Files processed: {stats.files_processed:,}")
        print(f"ğŸ”¬ Total resources extracted: {stats.total_resources:,}")
        print(f"ğŸ‘¥ Patients: {stats.total_patients:,}")
        print(f"ğŸ¥ Encounters: {stats.total_encounters:,}")
        print(f"ğŸ“Š Observations: {stats.total_observations:,}")
        print(f"ğŸ©º Conditions: {stats.total_conditions:,}")
        print(f"âš•ï¸ Procedures: {stats.total_procedures:,}")
        print(f"ğŸ’Š Medications: {stats.total_medications:,}")
        
        print(f"\nâš¡ Performance Metrics:")
        print(f"â±ï¸ Total processing time: {total_time_hours:.2f} hours")
        print(f"ğŸš€ Processing rate: {processing_rate:.1f} files/sec")
        print(f"ğŸ“ˆ Data throughput: {data_throughput:.2f} GB/sec")
        print(f"ğŸ’¾ Peak memory usage: {stats.memory_peak_mb:.1f} MB")
        
        # Parquet output summary
        print(f"\nğŸ“¦ Parquet Output Summary:")
        total_partitions = sum(info['partitions'] for info in partition_summary.values())
        total_output_gb = sum(info['total_size_mb'] for info in partition_summary.values()) / 1024
        
        print(f"ğŸ“‚ Total partitions created: {total_partitions:,}")
        print(f"ğŸ’¾ Total output size: {total_output_gb:.2f} GB")
        print(f"ğŸ“ Output location: /media/amansingh/data/fhir_test")
        
        print(f"\nğŸ“Š Resource Type Breakdown:")
        for resource_type, info in sorted(partition_summary.items()):
            if info['partitions'] > 0:
                print(f"  {resource_type}: {info['partitions']:,} partitions, {info['total_size_mb']:.1f} MB")
        
        # Knowledge graph summary
        if graph_summary.get('enabled'):
            print(f"\nğŸ•¸ï¸ Hierarchical Knowledge Graph Summary:")
            print(f"ğŸ”¢ Total entities: {graph_summary['total_entities']:,}")
            print(f"ğŸ”— Total relationships: {graph_summary['total_relationships']:,}")
            print(f"ğŸ“Š Graph levels: {len(graph_summary['levels'])}")
            
            print(f"\nğŸ“ˆ Graph Level Breakdown:")
            for level_name, level_info in graph_summary['levels'].items():
                if level_info['entities'] > 0:
                    print(f"  {level_name}: {level_info['entities']:,} entities, {level_info['relationships']:,} relationships")
            
            print(f"ğŸ’¾ Graph summary saved to: /media/amansingh/data/fhir_test/hierarchical_knowledge_graph_summary.json")
        
        # Error summary
        if stats.errors:
            print(f"\nâš ï¸ Errors Encountered: {len(stats.errors)}")
            error_types = {}
            for error in stats.errors:
                error_type = error.split(':')[0] if ':' in error else 'Unknown'
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                print(f"  {error_type}: {count} occurrences")
        
        print(f"\nğŸ¯ SUCCESS METRICS:")
        success_rate = (stats.files_processed / discovery['total_files']) * 100
        print(f"ğŸ“ˆ File processing success rate: {success_rate:.1f}%")
        print(f"ğŸ¯ Resource extraction rate: {stats.total_resources / stats.files_processed:.0f} resources/file")
        print(f"ğŸ’ª Memory efficiency: {stats.memory_peak_mb / 1024:.1f} GB peak / 16.0 GB limit = {(stats.memory_peak_mb / 1024 / 16.0) * 100:.1f}%")
        
        print(f"\n" + "=" * 80)
        print(f"ğŸ‰ FULL 435GB FHIR DATASET PROCESSING COMPLETE!")
        print(f"ğŸ“… End time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ Ready for distributed analytics and visualization!")
        print("=" * 80)
        
        return result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Processing interrupted by user")
        print(f"ğŸ“ Partial results may be available in: /media/amansingh/data/fhir_test")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"Critical error during processing: {e}")
        print(f"\nâŒ PROCESSING FAILED: {e}")
        print(f"ğŸ“ Check logs at: /media/amansingh/data/fhir_test/processing.log")
        print(f"ğŸ“ Partial results may be available in: /media/amansingh/data/fhir_test")
        sys.exit(1)

if __name__ == "__main__":
    # Check prerequisites
    source_dir = Path('/home/amansingh/dev/andola/healthcare/synthea/output/fhir')
    output_dir = Path('/media/amansingh/data/fhir_test')
    
    if not source_dir.exists():
        print(f"âŒ Error: Source directory not found: {source_dir}")
        sys.exit(1)
    
    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Run the processing
    result = main()